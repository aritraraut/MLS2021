{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "### Statement: under Gaussian assumption linear regression amounts to least square (ordinary least square).\n",
    "### Proof:\n",
    "Let us assume that the target variables and the inputs are related as, $$y_{i}=\\theta^{T}x_{i} + \\epsilon_{i}$$ \n",
    "Where, $\\epsilon_{i}$'s are random noise to model unknown effects and they are IID(independently and identically distriubuted) random variables.So,\n",
    "$$\\epsilon_{i} \\sim N(0,\\sigma^{2}),\\ [\\sigma=standard \\ deviation \\ of \\ normal \\ distribution]$$\n",
    "So, we know,\n",
    "$$ p(\\epsilon_{i})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-{\\frac{\\epsilon^{2}_{i}}{2\\sigma^{2}}})$$\n",
    "$$\\Rightarrow p(y_{i}-\\theta^{T}x_{i})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-{\\frac{y_{i}-\\theta^{T}x_{i}}{2\\sigma^{2}}})$$\n",
    "However, the conventional way to write the probability is,\n",
    "$$p(y_{i}|x_{i,j};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y_{i}-\\theta^{T}x_{i})^{2}}{2\\sigma^{2}})$$\n",
    "the notation indicates that this is the distribution of $x_{i}$ and $y_{i}$ and parameterized by $\\theta$.The probability of the data is given by $ p(\\bar{y}|X;\\theta)$.The quantity is typically viewed as a function of $\\bar{y}$,for fixed value of $\\theta$.when we wish to explicitly see this as function of $\\theta$,we call it the likelihood function.\n",
    "$$L(\\theta)=L(\\theta,x,\\bar{y})=p(\\bar{y}|X;\\theta)$$\n",
    "$$=\\prod^{m}_{i=1}p(y_{i}|x_{i};\\theta)$$\n",
    "$$=\\prod^{m}_{i=1}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y_{i}-\\theta^{T}x_i)^{2}}{2\\sigma^{2}})$$\n",
    "Now given this probabilistic model relating the $y_{i}$'s and the $x_{i}$'s,what is a reasonable way of choosing our best guess of parameters $\\theta$ ?.The principal of \\textbf{maximum likelihood} says that we should choose\n",
    "$\\theta$ so as to make the data as high probability as possible.So, we should maximize L($\\theta$).\n",
    "In perticular,the derivations will be a bit simpler if we maximize the \\thextbf{log likelihood}:\n",
    "$$\\ell(\\theta)=log L(\\theta)$$\n",
    "$$=log \\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y_{i}-\\theta^{T}x_{i})^{2}}{2\\sigma^{2}})$$\n",
    "$$=\\sum_{i=1}^{m}log(\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y_{i}-\\theta^{T}x_{i})^{2}}{2\\sigma^{2}}))$$\n",
    "$$=m\\ log\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^{2}}.\\frac{1}{2}\\sum_{i=1}^{m}(y_{i}-\\theta^{T}x_{i})^{2}$$\n",
    "$$=-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{m}(y_{i}-\\theta^{T}x_{i})^{2}$$\n",
    "Hence,under gaussian error assumption MLE with parameter $\\theta$,is same as least square minimization.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
